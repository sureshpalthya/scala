
import org.apache.spark.sql.execution.datasources.hbase._
import org.apache.spark.sql._
import org.apache.spark.sql.functions._

trait reportClassSkeleton {
  def mainMethod(sparkObject:SparkSession,configFilePath:String)={}
}



import com.hbase.catalog._
import org.apache.spark.sql.execution.datasources.hbase._
import org.apache.spark.sql._
import org.apache.spark.sql.functions._
import org.apache.hadoop.hbase.protobuf.ProtobufUtil
import org.apache.hadoop.hbase.protobuf.generated.ClientProtos
import org.apache.hadoop.hbase.util.Base64
import org.apache.hadoop.hbase.client._
import org.apache.hadoop.hbase.util.Bytes
import java.util.Properties
import java.io._
import com.sprint.edl.jdbc._
import scala.util.control._
import scala.util.control.Breaks._

class R12_InventoryReceiving_Newest extends reportClassSkeleton{
  override def mainMethod(sparkObject:SparkSession,configFilePath:String)={
   
   
   

val prop: Properties = new Properties()
val fileName = configFilePath+"R12_hbase_config_key.conf"
val hdfsConf = new org.apache.hadoop.conf.Configuration()
val fs = org.apache.hadoop.fs.FileSystem.get(hdfsConf)
val is = fs.open(new org.apache.hadoop.fs.Path(fileName))
prop.load(is)
println(" Loading prop file completed !!! ")
println(" Setting up application configurations !!! ")
val appName: String = prop.getProperty("report.application.name")
val columnfamily: String = prop.getProperty("hbase_column_family")
val scantable_logic: String = prop.getProperty("HbaseScan_By_time")

val cfDataBytes = Bytes.toBytes(columnfamily)
val hive_table_name = prop.getProperty("hive_report_name")
val hbase_omim_schema_name=prop.getProperty("hbase_omim_schema_name")
val cntLoop=prop.getProperty("cntlTbl_try_count").toInt
val outer = new Breaks;

//println("scantable_logic:"+scantable_logic)
println("Success fully Loaded Property File : "+fileName)


val application_id=sparkObject.sparkContext.applicationId

var cntlProp: Properties = new Properties()

val t1 = new cntlTable

if(cntLoop == null)
cntLoop==1
outer.breakable {
for (a <- 0 to cntLoop)
{
println("Control table loop iteration:"+a)
cntlProp=t1.setRepStartStatus(appName,application_id,prop)
println("report status output:"+cntlProp.getProperty("runStatus"))
if(cntlProp.getProperty("runStatus")=="success")
{
println("Control table insert status returned Success")
outer.break
}
}
}


//cntlProp=t1.setRepStartStatus(appName)
val repId=cntlProp.getProperty("id").toInt
println("Control table insert id:"+repId)

if (repId != -1)
{
try
{    
def convertScanToString(scan: Scan): String = {
val proto: ClientProtos.Scan = ProtobufUtil.toScan(scan);
return Base64.encodeBytes(proto.toByteArray());
}
val mmt_select=prop.getProperty("df_mmt.select")
val store_hierarchy_select =  prop.getProperty("df_store_hierarchy.select")
val sku_hierarchy_select= prop.getProperty("df_sku_hierarchy.select")
val employee_select= prop.getProperty("df_employee.select")
val df_union1_select =  prop.getProperty("df_union1.select")
val df_union2_select= prop.getProperty("df_union2.select")
val df_final_select= prop.getProperty("df_final.select")

val df_mmt_r12=sparkObject.sqlContext.sql(s"""$mmt_select""")
df_mmt_r12.createOrReplaceTempView("mmt_r12")
df_mmt_r12.cache()

val df_store_hierarchy_r12= sparkObject.sqlContext.sql(s"""$store_hierarchy_select""")
df_store_hierarchy_r12.createOrReplaceTempView("store_hierarchy_r12")
df_store_hierarchy_r12.cache()

val df_sku_hierarchy_r12=sparkObject.sqlContext.sql(s"""$sku_hierarchy_select""")
df_sku_hierarchy_r12.createOrReplaceTempView("sku_hierarchy_r12")
df_sku_hierarchy_r12.cache()

val df_employee_r12 = sparkObject.sqlContext.sql(s""" $employee_select """)
df_employee_r12.createOrReplaceTempView("employee_r12")

val df_union1_r12= sparkObject.sqlContext.sql(s""" $df_union1_select """)
df_union1_r12.createOrReplaceTempView("union1_r12")


val df_union2_r12= sparkObject.sqlContext.sql(s""" $df_union2_select """)
df_union2_r12.createOrReplaceTempView("union2_r12")

val df_final_r12= sparkObject.sqlContext.sql(s""" $df_final_select """)  
df_final_r12.createOrReplaceTempView("R12_final")
df_final_r12.cache()
df_final_r12.show()


val table_status =sparkObject.catalog.tableExists(hive_table_name)
if (table_status == true) {
//sparkObject.sqlContext.sql(s"""drop table if exists $hive_table_name """)
//sparkObject.sqlContext.sql(s"""create table $hive_table_name STORED AS ORC as select received_quantity, received_date, COALESCE(carton_number,pkg_tracking_number,'NA') PO, cost, sku, short_nm, class_nm, category,  dept_nm,  ORGANIZATION_CODE , receive_by_login , LOGIN_NAME, home_co_nm , region_co_nm , district_co_nm , store_co_nm , vendor, 0 rms_delivery_no from R12_final""")  
sparkObject.sqlContext.sql(s"""TRUNCATE TABLE $hive_table_name""")
sparkObject.sqlContext.sql(s"""INSERT OVERWRITE TABLE $hive_table_name partition(tran_dt) select received_quantity, received_date, COALESCE(carton_number,pkg_tracking_number,'NA') PO, cost, sku, short_nm, class_nm, category,  dept_nm,  ORGANIZATION_CODE , receive_by_login , LOGIN_NAME, home_co_nm , region_co_nm , district_co_nm , store_co_nm , vendor, 0 rms_delivery_no, cast(from_unixtime(unix_timestamp(received_date,'yyyy-MM-dd'),'yyyy-MM-dd') as date) as tran_dt from R12_final""")
sparkObject.sqlContext.sql(s"""ANALYZE TABLE $hive_table_name PARTITION(tran_dt) COMPUTE STATISTICS""")
}
else
{    
//spark.sqlContext.sql(s"""create table $hive_table_name STORED AS ORC as select received_quantity, received_date, COALESCE(carton_number,pkg_tracking_number,'NA') PO, cost, sku, short_nm, class_nm, category,  dept_nm,  ORGANIZATION_CODE , receive_by_login , LOGIN_NAME, home_co_nm , region_co_nm , district_co_nm , store_co_nm , vendor, 0 rms_delivery_no from R12_final""")
//spark.sqlContext.sql(s"""TRUNCATE TABLE $hive_table_name""")
sparkObject.sqlContext.sql(s"""select received_quantity, received_date, COALESCE(carton_number,pkg_tracking_number,'NA') PO, cost, sku, short_nm, class_nm, category,  dept_nm,  ORGANIZATION_CODE , receive_by_login , LOGIN_NAME, home_co_nm , region_co_nm , district_co_nm , store_co_nm , vendor, 0 rms_delivery_no, cast(from_unixtime(unix_timestamp(received_date,'yyyy-MM-dd'),'yyyy-MM-dd') as date) as tran_dt from R12_final""").write.format("orc").partitionBy("tran_dt").mode("overwrite").saveAsTable(hive_table_name)
sparkObject.sqlContext.sql(s"""ANALYZE TABLE $hive_table_name PARTITION(tran_dt) COMPUTE STATISTICS""")    
}

// sparkObject.catalog.dropTempView("store_hierarchy_r13")
// sparkObject.catalog.dropTempView("sku_hierarchy1_r13")
// sparkObject.catalog.dropTempView("sku_hierarchy2_r13")
// sparkObject.catalog.dropTempView("sku_hierarchy3_r13")
// sparkObject.catalog.dropTempView("sku_hierarchy4_r13")
// sparkObject.catalog.dropTempView("employee_r13")
// sparkObject.catalog.dropTempView("union1_r13")
// sparkObject.catalog.dropTempView("union2_r13")
// sparkObject.catalog.dropTempView("union3_r13")
// sparkObject.catalog.dropTempView("union4_r13")

}
catch {
case ex: Exception => {
println("Application caught an exception !!!")
val sw = new StringWriter
ex.printStackTrace(new PrintWriter(sw))
println(ex.printStackTrace())
val repStatus=t1.setRepEndStatus(appName,repId,"Failed",sw.toString.substring(0,1000),prop)
println("report status output:"+repStatus)
//System.exit(-999)
throw ex
}

} finally {
println("Application Exiting!!!")
println("Exiting finally...")
}


//if(cntLoop == null) cntLoop==1
outer.breakable
{
for (a <- 0 to cntLoop)
{
val repStatus=t1.setRepEndStatus(appName,repId,"Complete",null,prop)
println("omim update status output:"+cntlProp.getProperty("runStatus"))
if(repStatus)
{
println("Control table update status returned Success")
outer.break
}
}
}

//val repStatus=t1.setRepEndStatus(appName,repId,"Complete",null)
//println("omim update status output:"+cntlProp.getProperty("runStatus"))

}
else
println("Report"+appName+" is not ready for EDL aggregation")
   
  }
}
